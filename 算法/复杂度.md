# 为什么需要分析算法复杂度

其实我们可以通过实际运行代码来获取代码运行时的相关统计监测数据，从而分析算法的执行效率，这种方法称为**事后统计法**。然而这种方法具有很大的局限性。

##### 1、测试结果非常依赖测试环境

同样的代码在不同的环境中运行可能的的结果完全不同。

##### 2、测试数据会很大程度影响测试结果

同样的代码，同样的测试环境，如果测试数据不同得到的测试结果可能也会有很大的差异。比如同样的排序算法，待排序数据的有序度会很大程度的影响排序算法的效率。比如如果待排序数据已经是有序的，算法无需做任何操作，执行效率会很高。
待排序数据的数量规模也会对算法效率产生极大的影响。对于少量的数据，插入排序反倒比快速排序要快。
所以我们需要一个理论上的，无需实际运行代码就能估算算法效率的方法。

# 大O复杂度表示法

算法的执行效率，也就是代码的执行时间。


	int cal(int n) {
   		int sum = 0;
   		int i = 1;
   		for (; i <= n; ++i) {
     		sum = sum + i;
   		}
	}


比如上述代码，从CPU执行角度来看，每一行代码的实际执行时间可能都不同，但是由于只做理论上的分析统计，所以可以假设每行代码的执行时间都一样，记作unit_time。

于是对于上述代码：2、3行代码分别需要1个unit_time执行时间，4、5行都执行了n次，所以两行代码需要2n * unit_time时间来执行，所以这段代码的总执行时间为(2n + 2) * unit_time时间。

	int cal(int n) {
  		int sum = 0;
    	int i = 1;
    	int j = 1;
   		for (; i <= n; ++i) {
     		j = 1;
     		for (; j <= n; ++j) {
       			sum = sum +  i * j;
    		}
   		}
 	}

对于上述代码，跟之前一样的分析可以得到总执行时间为(2n²+2n+3)*unit_time。尽管不知道unit_time的实际值是多少，但是这都不重要，通过分析可以得知
**代码的执行时间与代码执行次数成正比**。

所以我们可以把这个规律用一个公式表示：
**T(n) = O(f(n))**
其中，T(n)表示代码执行时间，n表示数据规模大小，f(n)表示代码执行次数总和，O表示执行时间T(n)与f(n)成正比。

所以前面两段代码可以记为：T(n) = O(2n+2)和T(n) = O(2n²+2n+3)。这就是**大O时间复杂度表示法**。实际上，这种表示法并不代表代码的实际执行实现，而是表示**代码执行时间随数据规模增长的趋势**，因此也称为**渐进时间复杂度（asymptotic time complexity)**，简称**时间复杂度**。

其实对于上面的表示法，当n的数值很大时，公式中常量、低阶、系数三部分对于增长趋势的影响就很小了，可以忽略不计，我们只需要记录一个最大量级就可以了，于是对于之前提到的两段代码的时间复杂度可以记为：T(n) = O(n)和T(n) = O(n²)。

#时间复杂度分析
##### 1、只关注循环次数最多的代码
大O时间复杂度表示的是一种趋势，通常会忽略公式中的常量、低阶、系数，只需要记录一个最大量级就可以了。所以在**分析一段代码的时间复杂度时，也只需要关注代码中循环次数最多的代码就可以了**。所以，对于前面的两段代码，只需要统计for循环的时间就可以了。
##### 2、加法法则：总复杂度等于量级最大的那段代码的复杂度

	int cal(int n) {
   		int sum_1 = 0;
   		int p = 1;
   		for (; p < 100; ++p) {
     		sum_1 = sum_1 + p;
   		}

   		int sum_2 = 0;
   		int q = 1;
   		for (; q < n; ++q) {
     		sum_2 = sum_2 + q;
   		}
 
   		int sum_3 = 0;
   		int i = 1;
   		int j = 1;
   		for (; i <= n; ++i) {
     		j = 1; 
     		for (; j <= n; ++j) {
       			sum_3 = sum_3 +  i * j;
     		}
   		}
 
  		return sum_1 + sum_2 + sum_3;
 	}

上面的代码由三个部分组成，分别求sum_1，sum_2和sum_3。在分析这段代码的复杂度是可以先分别分析每部分代码的复杂度，然后再取其中量级最大的作为整段代码的时间复杂度。

对于求sum_1的代码块，这个for循环执行了100次，是一个常量，与n无关。对于与n无关的循环代码，无论循环次数有多大，都是一个已知数值，也就是将其时间复杂度当作常量处理。所以当n趋于无穷大时，就可以将常量部分忽略，尽管这可能对代码的执行时间统计有影响，但是时间复杂度表示并不是精确值，而是表示一种趋势，所以不管常量有多大都可以将其忽略。

对于求sum_2和sum_3的for循环代码，其时间复杂度分别为O(n)和O(n²)。

所以综合三部分代码的时间复杂度来看，最大量级为n²，所以对于整段代码的时间复杂度也就是O(n²)。所以**时间复杂度等于最大量级时间复杂度**。用公式表示为
**T(n) = O(max(f(n), g(n)))**。

##### 3、乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积
对于前面代码中求sum_3代码块来说，其时间复杂度为O(n²)，也就是外层循环的时间复杂度O(n)乘以内存循环的时间复杂度O(n)，即T(n) = O(n) * O(n) = O(n * n) = O(n²)。

# 几种常见时间复杂度
> 常量阶O(1)

> 对数阶O(logn)

> 线性阶O(n)

> 线性对数阶O(nlogn)

> 平方阶O(n²)

> 立方阶O(n³)

> ...

> k次方阶O(nK)

> 指数阶O(2N)

> 阶乘阶O(n!)
可以粗略的分为**多项式量级**和**非多项式量级**。其中非多项式量级只有O(2N)和O(n!)。

随着n的规模越来越大，非多项式量级的时间复杂度会变得越来越低效。

## 1、O(1)
O(1)只是常量级时间复杂度的一种表示方式，并不是指代码时间复杂度为1。比如下面的代码，即便代码有3行，但它的时间复杂度仍然记为O(1)。只要代码的执行时间与n无关，其时间复杂度都可以记为O(1)。**只要代码的执行时间不随n的变化而变化，即使代码有成千上万行，其时间复杂度都为O(1)**。

	int i = 8;
	int j = 6;
	int sum = i + j;

## 2、O(logn)、O(nlogn)

	i=1;
	while (i <= n)  {
  		i = i * 2;
	}

代码中i的取值从1开始，每次循环都乘以2，当i大于n时循环结束。实际上，i的取值就是一个等比数列。也就是求解2的x次方等于n，即x = log2n，所以这段代码的时间复杂度为O(log2n)。
如果上述代码中每次循环i都乘以3，那么其时间复杂度就是O(log3n)。实际上对于对数阶的复杂度，不管底数为几，都可以记为O(logn)。

为什么呢？对数是可以互相转换的，log3n = log32 * log2n，其中log32为常数，所以可以忽略，即**O(Cf(n)) = O(f(n))。所以O(log3n) = O(log2n)。因此，在对数阶的时间复杂度中，可以忽略对数的底，统一记为O(logn)。

如果一段代码的时间复杂度为O(logn)，其循环n次，那么代码的时间复杂度就是O(nlogn)。

# 空间复杂度
空间复杂度全称**渐进空间复杂度(asymptotic space complexity)**，**表示算法的执行所需的存储空间与数据规模之间的增长关系**。

	void print(int n) {
  		int i = 0;
  		int[] a = new int[n];
  		for (i; i <n; ++i) {
    		a[i] = i * i;
  		}

  		for (i = n-1; i >= 0; --i) {
    		print out a[i]
  		}
	}

对于上述代码，第二行中申请了一个空间存储变量i，但由于它是常量，同时间复杂度一样，可以忽略，在第三行代码中申请了一个长度为n的int数组，其他代码就没有申请存储空间，所以这段代码的空间复杂度就可以记作O(n)。

# 最好时间复杂度（best case time complexity）、最坏时间复杂度（worst time complextiy）

	int[] array = new int[n];
	
	int find(int x) {
		int pos = -1;
		for (int i = 0; i < n; i++) {
			if (array[i] == x) {
				pos = i;
				break;
			}
		}
		return pos;
	}

对于上面的代码，其时间复杂度为多少呢？当数组中第一个数就等于x时，代码只需要执行一次而不需要再遍历剩下的n - 1个数了，所以这时候时间复杂度为O(1)，但是如果数组中不存在等于x的数，就需要将整个数组遍历一次，因此时间复杂度为O(n)。所以在不同的情况下，这段代码的时间复杂度是不一样。因此需要引入新的概念：：**最好情况时间复杂度**、**最坏情况时间复杂度**。

**最好情况时间复杂度**：在最理想的情况下代码的时间复杂度。

**最坏情况时间复杂度**：在最坏的情况下代码的时间复杂度。

对于上述代码，其最好时间复杂度就为O(1)，最坏时间复杂度就是O(n)。

# 平均情况时间复杂度
最好情况时间复杂度和最坏情况时间复杂度都是在极端情况下才会发生的，发生的概率不大，所以并不能很好的代表代码的时间复杂度。因此为了更好的表示代码的时间复杂度，还需要引入一个概念：**平均情况时间复杂度**，简称**平均时间复杂度**。

以上述在数组中查找指定数值的代码为例。有n+1种情况：**x在数组的0 ~ n-1位置处**和**不在数组中**。把每种情况下需要遍历的元素个数累加起来然后除以n+1就可以得到需要遍历的元素个数的平均值。也就是（1 + 2 + 3 + ... + n + n）/ （n + 1）= n(n + 3) / 2(n + 1)。省略公式中的常量、低阶、系数后，时间复杂度可以记为O(n)。

通过这种方法计算得到的结果虽然是正确的，但是计算过程有点问题。因为对于n+1种情况，每种情况出现的概率并不是一样的。

对于x，要么在数组中，要么不在数组中，这两种情况的实际概率统计起来很麻烦，所以可以假设两种情况的概率都是50%。对于**x在数组的0 ~ n-1位置处**每种情况的概率也都可以假设为1/n。所以x出现在0 ~ n-1中任意位置处的概率为1/2n。

所以如果将每种情况发生的概率考虑进去，那么平均时间复杂度的计算就是：
1 * 1/2n + 2 * 1/2n + 3 * 1/2n + ... + n * 1/2n + n * 1/2 = (3n + 1)/4。这个值就是概率论中的**加权平均值**，也叫作**期望值**，所以平均时间复杂度全称应该是**加权平均时间复杂度**或者**期望时间复杂度**。

# 均摊时间复杂度

	int[] array = new int[n];
	int count = 0;
	
	void insert(int val) {
		if (count == array.length) {
			int sum = 0;
			for (int i = 0; i < array.length; i++) {
				sum += array[i];
			}
			array[0] = sum;
			count = 1;
		}
		
		array[count] = val;
		count++;
	}

在最理想的情况下，数组中有空闲空间，这时候只需要将数值插入数值下标为count的位置即可，所以最好时间复杂度就是O(1)。最坏情况下，数值中没有空闲空间了，这时候就需要先遍历数组求和，然后将求得的和插入数组中，所以最坏时间复杂度为O(n)。假设数组长度为n，根据插入位置可以分为n种情况，每种情况的时间复杂度都是O(1)，另外还有一种情况就是数组中没有空闲空间了，这时候时间复杂度为O(n)，这n+1种情况的概率都假设是1/(n+1),所以平均时间复杂度等于1 * 1/(n+1) + 1 * 1/(n+1) + ... + n * 1/(n+1)，即平均时间复杂度为O(1)。

对于上面的代码的平均时间复杂度计算并不需要这么复杂，因为在大部分情况下，insert方法的时间复杂度都是O(1)，只有在个别情况下时间复杂度才会变为O(n)。另外，对于两种复杂度出现的频率也是有规律的，在出现了一次O(n)后，后续的（n-1）次插入的时间复杂度都是O(1)。如此循环。所以，对于这种特殊场景的复杂度，可以引入一个新的概念：**均摊时间复杂度**，与之对于的分析方法：**平摊分析**。

因此，对于上述情况，一次O(n)后跟着n-1次O(1)，所以把耗时多的那次操作平摊到接下来的n-1次操作上，那么这段代码的均摊时间复杂度就是O(1)。

在能够应用均摊时间复杂度分析的场景，一般均摊时间复杂度等于最好时间复杂度。

均摊时间复杂度的应用场景比较特殊，在实际情况中并不常用。




# 快速排序算法复杂度分析

伪代码：

	QUICKSORT(A, p, r)
		q = PARTITION(A, p, r)
		QUICKSORT(A, p, q - 1)
		QUICKSORT(A, q + 1, r)

	PARTITION(A, p, r)
		x = A[r]
		i = p - 1
		for j = p to r - 1
			if A[j] <= x
				i = i + 1
				exchange A[i] with A[j]
		exchange A[i + 1] with A[r]
		return i + 1


快速排序算法最差时间复杂度O(n²)，最好时间复杂度O(nlogn)，平均时间复杂度O(nlogn)。

快速排序算法的运行时间依赖于划分是否平衡，而平衡与否又依赖于用于划分的元素。如果划分是平衡的，那么快排的性能与归并排序一样，如果划分是不平衡的，那么快排的性能接近于插入排序。

## 最坏情况划分

当划分产生的两个子数组分别包含n-1个元素和0个元素时，快排的最坏情况发生。假设每一次递归都出现了这种情况。划分操作的时间复杂度是O(n)。对于一个大小为0的数字进行递归调用会直接返回，所以T(0) = O(1)，于是算法运行时间可以表示为：

	T(n) = T(n - 1) + T(0) + O(n) = T(n - 1) + O(n) = O(n²)

此外，当输入数组已经完全有序时，快速排序的时间复杂度仍然为O(n²)。

## 最好情况划分

在可能的最平衡的划分中，PARTITION得到的两个子数组的规模都不大于n/2，因为其中一个子数组的规模为n/2，另一个为（n/2 - 1）。此时快速排序的时间复杂度为

	T(n) = 2T(n/2) + O(n) = O(nlogn)

快速排序算法的平均时间复杂度更接近于它的最好时间复杂度。只要划分是常数比例的，算法的运行时间总是O(nlogn)。

## 对于平均情况的直观观察

快速排序的行为依赖于输入数组中元素的值得相对顺序，而不是某些特定值本身。假设输入数据的所有排列都是等概率的。

当对一个随机输入的数组进行快速排序时，在每一层上都有同样的划分是不太可能的，在平均情况下，PARTITION所产生的划分同时混合有好和差的划分，好和差是随机分布的。假设好和差的划分交替出现在递归树的各层上，并且好的划分是最好情况，差的划分是最差情况。在递归树的连续两层上，在根节点，划分代价为n，划分产生了两个大小分别为n-1和0的数组，即最坏情况，在下一层上，大小为n-1的数组按最好情况划分为（n-1)/2 - 1和（n-1）/2两个子数组。这时就产生了三个子数组，大小分别为0，（n-1）/2-1，（n-1）/2，此时划分的代价为O(n)+O(n-1)=O(n)。而当第一层划分就是最好情况时，即第一层就划分为（n-1）/2两个子数组，此时的划分代价仍然为O(n)。从直观上，差划分的代价可以被吸收到好划分的代价中，而得到的划分结果也是好的。因此，当好和差的划分交替出现时，快速排序的时间复杂度与全是好的划分是一样，仍然为O(nlogn)。区别只是O种隐含的常熟因子略大一些。

## 快速排序的随机化版本

伪代码：

	RANDOMIZED-PARTITION(A, p, r)
		i = RANDOM(p, r)
		exchange A[r] with A[i]
		return PARTITION(A, p, r)

	RANDOMIZED-QUICKSORT(A, p, r)
		if p < r
			q = RANDOMIZED-PARTITION(A, p, r)
			RANDOMIZED-QUICKSORT(A, p, q - 1)
			RANDOMIZED-QUICKSORT(A, q + 1, r)

与始终采用A[r]作为主元的方法不同，从子数组种随机选择一个元素作为主元，这样可以保证主元元素x=A[r]是等概率的从子数组的r-p+1个元素中选取的。因为主元元素是随机选取的，在平均情况下，对输入数组的划分是比较均衡的。

# 最坏情况分析

	T(n) = max(T(q) + T(n - q - 1)) + O(n)

因为PARTITION函数生成的两个子数组的规模和总为n-1，所以q的变化范围是0到n-1。猜测T(n)<=cn²成立，其中c为常数。带入递归式种得

	T(n) <= max(cq² + c(n - q - 1)²) + O(n) = c·max(q² + (n - q - 1)²) + O(n)

当q在端点上时取得最大值，所以max(cq² + c(n - q - 1)²)<=(n-1)²=n²-2n+1，代入后得到

	T(n) <= cn² - c(2n - 1) + 0(n) <= cn²

因为可以选择一个足够大的常数c，是的c(2n-1)项能显著大于O(n)项，所以有T(n)=O(n²)。

# 平均情况分期（期望情况分析）
QUICKSORT的运行时间由在PARTITION操作上花费的时间决定。每次对PARTITION的调用都会选择一个元素作为主元元素，而且该元素不会被包含在后续的对QUICKSORT和PARTITION的递归调用种，因此，在快速排序的整个执行期间，至多只能调用PARTITION操作n次。调用一次PARTITION的时间为O(1)再加上一段循环时间。这段时间与for循环的迭代次数成正比。for循环的每一轮迭代都要进行一次比较：比较主元元素与数组A中的另一个元素。如果可以统计比较操作被执行的总次数就能给出QUICKSORT的执行过程中，for循环所花时间的界了。

`当在一个包含n个元素的数组上进行QUICKSORT时，假设在PARTITION的比较操作的次数为X，那么QUICKSORT的运行时间为O(n+X)`

因此只需要计算出X就能知道快速排序的时间复杂度了。

将数组的各个元素重命名为z1, z2, ..., zn，其中zi是第i小的元素。定义Zij = {zi, z(i+1), ..., zj}为zi与zj之间的元素集合。算法什么时候比较zi和zj呢？

每一对元素之多比较一次。因为各个元素只与主元元素进行比较，并且在PARTITION调用结束后，该次调用种使用的主元元素就再也不会与任何其他元素进行比较了。

定义Xij = I{zi与zj进行比较}，因此算法的总比较次数为：

	X = ∑(i=1, n-1)∑(j=i+1, n)Xij

对上式取期望值：

	E(X) = E(∑(i=1, n-1)∑(j=i+1, n)Xij) = ∑(i=1, n-1)∑(j=i+1, n)Pr{zi与zj进行比较}

假设RANDOMIZED-PARTITION随机且独立的选择主元元素。考虑两个元素何时`不会`进行比较的情况。例如输入一个由数字1到10所构成的数组，元素顺序是随机的，假设第一个主元元素是7。那么，第一次调用PARTITION后就得到两个集合{1, 2, 3, 4, 5, 6}和{8， 9， 10}，这一过程种主元元素7要和其他所有元素进行一次比较，但是第一个集合中的任一元素没有与第二集合中的任何元素进行比较。

假设每个元素都是不同的，因此，一旦一个满足zi < x < zj的主元元素x被选择后，zi和zj就再也不会进行比较了。如果zi在Zij中的其他所有元素之前被选择作为主元，那么zi就会与Zij中除了自身外其他所有元素进行比较，类似的，如果zj在Zij中的其他所有元素之前被选为主元，那么zj就会与Zij中除了自身外其他所有元素进行比较。因此，**zi与zj会进行比较当且仅当Zij中被选为主元的元素为zi或zj**。

在Zij中的某个元素被选为主元前，整个集合Zij都属于某一划分的同一分区。因为Zij中有j-i+1个元素，并且选取主元是随机且独立的，因此，任何元素被选为主元的概率是1/(j-i+1)。

	Pr{zi与zj进行比较} = Pr{zi或zj是Zij中的第一个主元}
					 = Pr{zi是第一个主元} + Pr{zj是第一个主元}
					 = 1/(j-i+1) + 1/(j-i+1) = 2/(j-i+1)

因此

	E(X) = ∑(i=1, n-1)∑(j=i+1, n)Pr{zi与zj进行比较} = ∑(i=1, n-1)∑(j=i+1, n)(2/(j-i+1))

定义k=j-1，于是有

	E(X) = ∑(i=1, n-1)∑(j=i+1, n)(2/(j-i+1))
		= ∑(i=1, n-1)∑(k=1, n-i)(2/(k+1))
	 	< ∑(i=1, n-1)∑(k=1, n)(2/k)
	 	= ∑(i=1, n-1)O(logn)
	 	= O(nlogn)
